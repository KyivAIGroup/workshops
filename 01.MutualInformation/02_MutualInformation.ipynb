{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.MutualInformation.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "plV6Oi_Vg3Eb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mutual Information\n",
        "\n",
        "Mutual information between two random variables $X$ and $Y$ defines how well $X$ explains $Y$ and vice versa.\n",
        "\n",
        "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) \\tag{1}$$\n",
        "\n",
        "where $H(X)$ is the entropy of $X$ and $H(X|Y)$ is the conditional entropy of $X$ given $Y$. As it follows from $(1)$, mutual information is symmetric: $I(X; Y) = I(Y; X)$.\n",
        "\n",
        "$ H(X) = \\sum_{x} p(x) \\log_2 {1 \\over p(x)} \\\\ H(X|Y) = \\sum_y p(y)H(X|Y=y) = \\sum_y p(y) \\sum_x p(x|y)log_2{1 \\over p(x|y)}$\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1200px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)\n",
        "\n",
        "In the first task we'll examine the mutual information between\n",
        "\n",
        "* independent variables $X$ and $Y$\n",
        "* when $Y$ is a deterministic function of $X$\n",
        "* when $Y$ is a deterministic function of $X$ plus some noise"
      ]
    },
    {
      "metadata": {
        "id": "vNBIIeS8i1tB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "def discrete_entropy(digits) -> float:\n",
        "    \"\"\"\n",
        "    :param digits: realizations of a discrete random variable X\n",
        "    :return estimated entropy of X in bits\n",
        "    \"\"\"\n",
        "    unique, counts = np.unique(digits, return_counts=True)\n",
        "    proba = counts / len(digits)\n",
        "    entropy = sum(proba * np.log2(1 / proba))\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def discrete_mutual_information(x, y) -> float:\n",
        "    \"\"\"\n",
        "    :param x: realizations of a discrete random variable X\n",
        "    :param y: realizations of a discrete random variable Y\n",
        "    :return: estimated mutual information between X and Y in bits\n",
        "    \"\"\"\n",
        "    # your code goes here\n",
        "    entropy_x = discrete_entropy(x)\n",
        "    entropy_x_given_y = 0\n",
        "    for y_unique in np.unique(y):\n",
        "        mask = y == y_unique\n",
        "        x_given_y = x[mask]\n",
        "        entropy_x_given_y += discrete_entropy(x_given_y) * np.mean(mask)\n",
        "    info = entropy_x - entropy_x_given_y\n",
        "    return info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b4yfmtZHS2ro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9dcf00b8-adc5-449f-c26d-488c84ba0975"
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def test(size=1000, y_func=lambda x: x**2):\n",
        "    information = defaultdict(list)\n",
        "    y_entropy = defaultdict(list)\n",
        "    x_entropy = []\n",
        "    for trial in range(10):\n",
        "        x = np.random.randint(low=0, high=10, size=size)\n",
        "\n",
        "        y_random = np.random.randint(low=53, high=53 + 5, size=size)\n",
        "        y_deterministic = y_func(x)\n",
        "        noise = np.random.randint(low=0, high=10, size=size)\n",
        "        y_noisy = y_deterministic + noise\n",
        "\n",
        "        information['random'].append(discrete_mutual_information(x, y_random))\n",
        "        information['deterministic'].append(discrete_mutual_information(x, y_deterministic))\n",
        "        information['noisy'].append(discrete_mutual_information(x, y_noisy))\n",
        "\n",
        "        x_entropy.append(discrete_entropy(x))\n",
        "        y_entropy['random'].append(discrete_entropy(y_random))\n",
        "        y_entropy['deterministic'].append(discrete_entropy(y_deterministic))\n",
        "        y_entropy['noisy'].append(discrete_entropy(y_noisy))\n",
        "    x_entropy = np.mean(x_entropy)\n",
        "    for experiment_name in information.keys():\n",
        "        max_information = min(x_entropy, np.mean(y_entropy[experiment_name]))\n",
        "        print(f\"{experiment_name}: I(X; Y) = {np.mean(information[experiment_name]):.4f} \"\n",
        "              f\"± {np.std(information[experiment_name]):.4f} (maximum possible {max_information:.4f})\")\n",
        "\n",
        "test()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random: I(X; Y) = 0.0282 ± 0.0080 (maximum possible 2.3191)\n",
            "deterministic: I(X; Y) = 3.3155 ± 0.0036 (maximum possible 3.3155)\n",
            "noisy: I(X; Y) = 2.7591 ± 0.0189 (maximum possible 3.3155)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}