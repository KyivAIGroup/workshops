{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.MutualInformation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wHh6H6SAq92l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Up-to-date version: https://colab.research.google.com/drive/1CEQXZz8Ib5fH9X2PMFKpDLhGy6Immftl"
      ]
    },
    {
      "metadata": {
        "id": "plV6Oi_Vg3Eb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mutual Information\n",
        "\n",
        "Mutual information between two random variables $X$ and $Y$ defines how well $X$ explains $Y$ and vice versa.\n",
        "\n",
        "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) \\tag{1}$$\n",
        "\n",
        "where $H(X)$ is the entropy of $X$ and $H(X|Y)$ is the conditional entropy of $X$ given $Y$. As it follows from $(1)$, mutual information is symmetric: $I(X; Y) = I(Y; X)$.\n",
        "\n",
        "$ H(X) = \\sum_{x} p(x) \\log_2 {1 \\over p(x)} \\\\ H(X|Y) = \\sum_y p(y)H(X|Y=y) = \\sum_y p(y) \\sum_x p(x|y)log_2{1 \\over p(x|y)}$\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1200px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)\n",
        "\n",
        "In the first task we'll examine the mutual information between\n",
        "\n",
        "* independent variables $X$ and $Y$\n",
        "* when $Y$ is a deterministic function of $X$\n",
        "* when $Y$ is a deterministic function of $X$ plus some noise\n",
        "\n",
        "Let's start simple. What's the information between $X$ and $Y$ when\n",
        "\n",
        "```\n",
        "X = [5, 5, 8]\n",
        "Y = [3, 7, 7]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "vNBIIeS8i1tB",
        "colab_type": "code",
        "outputId": "d4e52276-88bb-4b14-fad8-3845fce57fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "def discrete_entropy(digits) -> float:\n",
        "    \"\"\"\n",
        "    :param digits: realizations of a discrete random variable X\n",
        "    :return estimated entropy of X in bits\n",
        "    \"\"\"\n",
        "    unique, counts = np.unique(digits, return_counts=True)\n",
        "    proba = counts / len(digits)\n",
        "    entropy = sum(proba * np.log2(1 / proba))\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def discrete_mutual_information(x, y) -> float:\n",
        "    \"\"\"\n",
        "    :param x: realizations of a discrete random variable X\n",
        "    :param y: realizations of a discrete random variable Y\n",
        "    :return: estimated mutual information between X and Y in bits\n",
        "    \"\"\"\n",
        "    # your code goes here\n",
        "    # I(X; Y) = H(X) - H(X|Y)\n",
        "    entropy_x = discrete_entropy(x)\n",
        "    entropy_x_given_y = 0\n",
        "    for y_unique in set(y):\n",
        "        mask = y == y_unique\n",
        "        x_given_y = x[mask]\n",
        "        entropy_x_given_y += discrete_entropy(x_given_y) * np.mean(mask)\n",
        "    mutual_information = entropy_x - entropy_x_given_y\n",
        "    return mutual_information\n",
        "\n",
        "\n",
        "xs = np.array([5, 5, 8])\n",
        "ys = np.array([3, 7, 7])\n",
        "print(f\"Mutual information between X={xs} and Y={ys} is {discrete_mutual_information(xs, ys):.3f} bits\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mutual information between X=[5 5 8] and Y=[3 7 7] is 0.252 bits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b4yfmtZHS2ro",
        "colab_type": "code",
        "outputId": "878b9236-6794-4779-8f27-f0234f253751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def test(size=1000, y_func=lambda x: x**2):\n",
        "    information = defaultdict(list)\n",
        "    y_entropy = defaultdict(list)\n",
        "    x_entropy = []\n",
        "    for trial in range(10):\n",
        "        x = np.random.randint(low=0, high=10, size=size)\n",
        "\n",
        "        y_random = np.random.randint(low=53, high=53 + 5, size=size)\n",
        "        y_deterministic = y_func(x)\n",
        "        noise = np.random.randint(low=0, high=10, size=size)\n",
        "        y_noisy = y_deterministic + noise\n",
        "\n",
        "        information['random'].append(discrete_mutual_information(x, y_random))\n",
        "        information['deterministic'].append(discrete_mutual_information(x, y_deterministic))\n",
        "        information['noisy'].append(discrete_mutual_information(x, y_noisy))\n",
        "\n",
        "        x_entropy.append(discrete_entropy(x))\n",
        "        y_entropy['random'].append(discrete_entropy(y_random))\n",
        "        y_entropy['deterministic'].append(discrete_entropy(y_deterministic))\n",
        "        y_entropy['noisy'].append(discrete_entropy(y_noisy))\n",
        "    x_entropy = np.mean(x_entropy)\n",
        "    for experiment_name in information.keys():\n",
        "        max_information = min(x_entropy, np.mean(y_entropy[experiment_name]))\n",
        "        print(f\"{experiment_name}: I(X; Y) = {np.mean(information[experiment_name]):.4f} \"\n",
        "              f\"± {np.std(information[experiment_name]):.4f} (maximum possible {max_information:.4f})\")\n",
        "\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random: I(X; Y) = 0.0275 ± 0.0054 (maximum possible 2.3194)\n",
            "deterministic: I(X; Y) = 3.3157 ± 0.0023 (maximum possible 3.3157)\n",
            "noisy: I(X; Y) = 2.7564 ± 0.0294 (maximum possible 3.3157)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MgNsh6il262i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One of the greatest advantages of using mutual information is that it captures nonlinear interactions, and it does not require assumptions about the structure of the underlying data (i.e., it is model independent). You can try different `y_func` in the test above, but make sure both $X$'s and $Y$'s are discrete. Otherwise, you have to cluster continuous data in bins (subject of 4th lesson).\n"
      ]
    }
  ]
}